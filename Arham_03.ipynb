{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Naive Bayes Classifier on Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muhammad Arham\n",
    "### 10016866912"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing & Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/18262293/how-to-open-every-file-in-a-folder\n",
    "#---------------------------------------------------------------------------------------------------------#\n",
    "#Loading Train Data\n",
    "#importing from the pos folder\n",
    "cwd = \"C:\\\\Users\\\\Arham\\\\Documents\\\\UTA\\\\SPRING 2020\\\\DM\\\\Project\\\\aclImdb_v1.tar\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\pos\"\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "row = []\n",
    "for filename in os.listdir(cwd):\n",
    "    with open(os.path.join(cwd, filename), 'r', encoding=\"utf8\") as f: # open in readonly mode\n",
    "        #print (filename + \",\" + f.read() + \",\" + \"1\")\n",
    "        col = []\n",
    "        col.append(filename)\n",
    "        col.append(f.read())\n",
    "        col.append(\"1\")\n",
    "        row.append(col)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------#\n",
    "#importing from the neg folder\n",
    "cwd = \"C:\\\\Users\\\\Arham\\\\Documents\\\\UTA\\\\SPRING 2020\\\\DM\\\\Project\\\\aclImdb_v1.tar\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\neg\"               \n",
    "for filename in os.listdir(cwd):\n",
    "    with open(os.path.join(cwd, filename), 'r', encoding=\"utf8\") as f: # open in readonly mode\n",
    "        #print (filename + \",\" + f.read() + \",\" + \"1\")\n",
    "        col = []\n",
    "        col.append(filename)\n",
    "        col.append(f.read())\n",
    "        col.append(\"0\")\n",
    "        row.append(col)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(row,columns =['File','Review','Class'])\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "#Class 1 represents all the reviews from the positive foler and Class 0 represents reviews from negative folder\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Loading Test data\n",
    "\n",
    "cwd = \"C:\\\\Users\\\\Arham\\\\Documents\\\\UTA\\\\SPRING 2020\\\\DM\\\\Project\\\\aclImdb_v1.tar\\\\aclImdb_v1\\\\aclImdb\\\\test\\\\pos\"\n",
    "\n",
    "row = []\n",
    "for filename in os.listdir(cwd):\n",
    "    with open(os.path.join(cwd, filename), 'r', encoding=\"utf8\") as f: # open in readonly mode\n",
    "        #print (filename + \",\" + f.read() + \",\" + \"1\")\n",
    "        col = []\n",
    "        col.append(filename)\n",
    "        col.append(f.read())\n",
    "        col.append(\"1\")\n",
    "        row.append(col)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------#\n",
    "#importing from the neg folder\n",
    "cwd = \"C:\\\\Users\\\\Arham\\\\Documents\\\\UTA\\\\SPRING 2020\\\\DM\\\\Project\\\\aclImdb_v1.tar\\\\aclImdb_v1\\\\aclImdb\\\\test\\\\neg\"               \n",
    "for filename in os.listdir(cwd):\n",
    "    with open(os.path.join(cwd, filename), 'r', encoding=\"utf8\") as f: # open in readonly mode\n",
    "        #print (filename + \",\" + f.read() + \",\" + \"1\")\n",
    "        col = []\n",
    "        col.append(filename)\n",
    "        col.append(f.read())\n",
    "        col.append(\"0\")\n",
    "        row.append(col)\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(row,columns =['File','Review','Class'])\n",
    "\n",
    "#test_df.head()\n",
    "\n",
    "#Class 1 represents all the reviews from the positive foler and Class 0 represents reviews from negative folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regex to clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "\n",
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3)\n",
      "(5000, 3)\n",
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "#x = df.iloc[:,1:2] #Review column\n",
    "#y = df[\"Class\"] #Class column\n",
    "\n",
    "#print(x.size)\n",
    "\n",
    "#80/20 split\n",
    "#x_develop,x_train,y_develop,y_train=train_test_split(x,y,test_size=0.2,random_state=np.random)\n",
    "\n",
    "train,develop  = train_test_split(df, test_size=0.2)\n",
    "\n",
    "type(train)\n",
    "print(train.shape)\n",
    "print(develop.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary List for all occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.w3resource.com/python-exercises/string/python-data-type-string-exercise-12.php\n",
    "\n",
    "def new_word_count(df):\n",
    "    counts = dict()\n",
    "    counts.update( {'total_pos' : 0} )\n",
    "    counts.update( {'total_neg' : 0} )\n",
    "    for index, row in df.iterrows():\n",
    "        #print (row[\"Review\"])\n",
    "        row[\"Review\"] = cleanhtml(row[\"Review\"])\n",
    "        row[\"Review\"] = re.sub('\\S+', lambda m: re.sub('^\\W+|\\W+$', '', m.group()), row[\"Review\"])\n",
    "        row[\"Review\"] = row[\"Review\"].lower()\n",
    "        words = row[\"Review\"].split()\n",
    "        explored_words = []\n",
    "        if row['Class']=='1':\n",
    "            counts['total_pos'] += 1\n",
    "        else:\n",
    "            counts['total_neg'] += 1\n",
    "        for word in words:\n",
    "            if word in counts:\n",
    "                if word in explored_words:\n",
    "                    continue\n",
    "                else:    \n",
    "                    counts[word] += 1\n",
    "                    explored_words.append(word)\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "                explored_words.append(word)\n",
    "            \n",
    "    return counts\n",
    "\n",
    "    \n",
    "    \n",
    "train_vocList = new_word_count(train)\n",
    "develop_vocList = new_word_count(develop)\n",
    "test_vocList = new_word_count(test_df)\n",
    "#train_vocList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://thispointer.com/python-filter-a-dictionary-by-conditions-on-keys-or-values/\n",
    "#https://kite.com/python/answers/how-to-remove-multiple-keys-from-a-dictionary-in-python\n",
    "import copy\n",
    "\n",
    "def filterRare(vocList,minOccurence):\n",
    "    newList = copy.deepcopy(vocList)\n",
    "    keys_to_remove = []\n",
    "    for (key, value) in newList.items():\n",
    "       # Check if key is even then add pair to new dictionary\n",
    "       if value < minOccurence:\n",
    "            keys_to_remove.append(key)\n",
    "    \n",
    "    #print(keys_to_remove)        \n",
    "    \n",
    "    for key in keys_to_remove:\n",
    "        del newList[key]\n",
    "\n",
    "    return newList\n",
    "\n",
    "    #print('Filtered Dictionary : ')\n",
    "    #print(newDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Words in original train dictionary: 107183\n",
      "No. of Words, in train, which have frequency > 5: 26694\n",
      "No. of Words in original develop dictionary: 51042\n",
      "No. of Words, in develop, which have frequency > 5: 11660\n",
      "No. of Words in original test dictionary: 118698\n",
      "No. of Words,in test, which have frequency > 5: 29556\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Words in original train dictionary: \"+str(len(train_vocList)))\n",
    "train_vocList = filterRare(train_vocList,5);\n",
    "print(\"No. of Words, in train, which have frequency > 5: \"+str(len(train_vocList)))\n",
    "print(\"No. of Words in original develop dictionary: \"+str(len(develop_vocList)))\n",
    "develop_vocList = filterRare(develop_vocList,5);\n",
    "print(\"No. of Words, in develop, which have frequency > 5: \"+str(len(develop_vocList)))\n",
    "print(\"No. of Words in original test dictionary: \"+str(len(test_vocList)))\n",
    "test_vocList = filterRare(test_vocList,5);\n",
    "print(\"No. of Words,in test, which have frequency > 5: \"+str(len(test_vocList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of the occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[the]: 19840/20000=0.992\n"
     ]
    }
   ],
   "source": [
    "def calc_probability(word,dataset,vocList):          \n",
    "    prob = vocList[word]/len(dataset)\n",
    "    print(\"P[\"+word+\"]: \"+str(vocList[word])+\"/\"+str(len(dataset))+\"=\"+str(prob))\n",
    "    return prob\n",
    "\n",
    "#P[“the”] = num of documents containing ‘the’ / num of all documents\n",
    "calc_probability(\"the\",train,train_vocList)\n",
    "\n",
    "def calc_probability_smoothing(word,dataset,vocList, laplaceVal):\n",
    "    prob_smooth = (vocList[word] + laplaceVal) / (len(dataset) + len(vocList) * laplaceVal)\n",
    "    print(\"P[\"+word+\"]: \"+str(vocList[word]+ laplaceVal)+\"/\"+str(len(dataset)+ len(vocList) * laplaceVal)+\", with smoothing= \"+str(prob_smooth))\n",
    "    return prob_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probability based on the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[the|Positive ]: 9907/9995= 0.9911955977988994\n"
     ]
    }
   ],
   "source": [
    "def conditional_probability(cond_word, dataset, vocList,condition):\n",
    "    count = 0\n",
    "    for index, row in dataset.iterrows():\n",
    "        words_in_review = row[\"Review\"].split()\n",
    "        for word in words_in_review:\n",
    "            if(word == cond_word and row[\"Class\"]== condition ):\n",
    "                count +=1\n",
    "                break\n",
    "    target = \"Positive\"\n",
    "    if (condition == '1'):\n",
    "        total = vocList['total_pos']\n",
    "    else:\n",
    "        total = vocList['total_neg']\n",
    "        target = \"Negative\"\n",
    "    \n",
    "    condProb = count/total\n",
    "    print(\"P[\"+cond_word+\"|\"+target+\" ]: \"+str(count)+\"/\"+str(total)+\"= \"+str(condProb))\n",
    "    \n",
    "    return condProb\n",
    "\n",
    "\n",
    "#P[“the” | Positive]  = # of positive documents containing “the” / num of all positive review documents\n",
    "conditional_probability('the',train,train_vocList,'1')\n",
    "\n",
    "\n",
    "def conditional_probability_smoothing(cond_word, dataset, vocList,condition,laplaceVal):\n",
    "    count = 0\n",
    "    for index, row in dataset.iterrows():\n",
    "        words_in_review = row[\"Review\"].split()\n",
    "        for word in words_in_review:\n",
    "            if(word == cond_word and row[\"Class\"]== condition ):\n",
    "                count +=1\n",
    "                break\n",
    "    target = \"Positive\"\n",
    "    if (condition == '1'):\n",
    "        total = vocList['total_pos']\n",
    "    else:\n",
    "        total = vocList['total_neg']\n",
    "        target = \"Negative\"\n",
    "    \n",
    "    sCondProb = (count+laplaceVal)/(total + laplaceVal * len(vocList))\n",
    "    print(\"P[\"+cond_word+\"|\"+target+\" ]: \"+str(count+laplaceVal)+\"/\"+str(total + laplaceVal * len(vocList))+\", with smoothing= \"+str(sCondProb))\n",
    "    \n",
    "    return sCondProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[the|Positive ]: 9907/9995= 0.9911955977988994\n",
      "P[the]: 19840/20000=0.992\n",
      "P[Positive|the]: 0.49934475806451617\n",
      "----------------------------------------------------\n",
      "P[the|Negative ]: 9933/10005= 0.9928035982008996\n",
      "P[the]: 19840/20000=0.992\n",
      "P[Negative|the]: 0.5006552419354838\n"
     ]
    }
   ],
   "source": [
    "#P(Class|word) = (P(Class)*P(word|Class))/P(word)\n",
    "def classifier(vocList,condition, cWord, dataset):\n",
    "    target = 'Positive'\n",
    "    if (condition == '1'):\n",
    "        totalClass = vocList['total_pos']\n",
    "    else:\n",
    "        target = 'Negative'\n",
    "        totalClass = vocList['total_neg']\n",
    "    \n",
    "    classProbability = totalClass/ len(dataset)\n",
    "    #print(\"Probability of Class \"+target +\": \"+str(classProbability))\n",
    "    \n",
    "    condProb = conditional_probability(cWord,dataset,vocList,condition)\n",
    "    \n",
    "    probabilityWord = calc_probability(cWord,dataset,vocList)\n",
    "    \n",
    "    prob_class_given_word = (classProbability * condProb)/probabilityWord\n",
    "    print(\"P[\"+target+\"|\"+cWord+\"]: \"+str(prob_class_given_word))\n",
    "    \n",
    "    return classProbability\n",
    "    \n",
    "    \n",
    "\n",
    "classifier(train_vocList,'1','the',train)\n",
    "print(\"----------------------------------------------------\")\n",
    "classifier(train_vocList,'0','the',train)\n",
    "\n",
    "\n",
    "#P(Class|word) = (P(Class)*P(word|Class))/P(word)\n",
    "def classifier_smoothing(vocList,condition, cWord, dataset,laplaceVal):\n",
    "    target = 'Positive'\n",
    "    if (condition == '1'):\n",
    "        totalClass = vocList['total_pos']\n",
    "    else:\n",
    "        target = 'Negative'\n",
    "        totalClass = vocList['total_neg']\n",
    "    \n",
    "    classProbability = (totalClass+ laplaceVal)/ (len(dataset) + laplaceVal*2)\n",
    "    #print(\"Probability of Class \"+target +\": \"+str(classProbability))\n",
    "    \n",
    "    condProb = conditional_probability_smoothing(cWord,dataset,vocList,condition,laplaceVal)\n",
    "    \n",
    "    probabilityWord = calc_probability_smoothing(cWord,dataset,vocList,laplaceVal)\n",
    "    \n",
    "    sProb_class_given_word = (classProbability * condProb)/probabilityWord\n",
    "    print(\"P[\"+target+\"|\"+cWord+\"]: \"+str(sProb_class_given_word))\n",
    "    \n",
    "    return classProbability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy using dev dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Five fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "No. of Words in dataset vocaublary list: 21206\n",
      "No. of Words, which have frequency > 5: 3627\n",
      "P[the|Positive ]: 488/492= 0.991869918699187\n",
      "P[the]: 993/1000=0.993\n",
      "P[Positive|the]: 0.4914400805639476\n",
      "-------------------------------------------------------------\n",
      "No. of Words in dataset vocaublary list: 20498\n",
      "No. of Words, which have frequency > 5: 3516\n",
      "P[the|Positive ]: 486/492= 0.9878048780487805\n",
      "P[the]: 991/1000=0.991\n",
      "P[Positive|the]: 0.49041372351160445\n",
      "-------------------------------------------------------------\n",
      "No. of Words in dataset vocaublary list: 21381\n",
      "No. of Words, which have frequency > 5: 3705\n",
      "P[the|Positive ]: 508/520= 0.9769230769230769\n",
      "P[the]: 983/1000=0.983\n",
      "P[Positive|the]: 0.5167853509664293\n",
      "-------------------------------------------------------------\n",
      "No. of Words in dataset vocaublary list: 21206\n",
      "No. of Words, which have frequency > 5: 3786\n",
      "P[the|Positive ]: 489/494= 0.9898785425101214\n",
      "P[the]: 992/1000=0.992\n",
      "P[Positive|the]: 0.49294354838709675\n",
      "-------------------------------------------------------------\n",
      "No. of Words in dataset vocaublary list: 21373\n",
      "No. of Words, which have frequency > 5: 3745\n",
      "P[the|Positive ]: 502/507= 0.9901380670611439\n",
      "P[the]: 991/1000=0.991\n",
      "P[Positive|the]: 0.5065590312815338\n",
      "K-fold cross validation with K as 5:0.5045999999999999\n"
     ]
    }
   ],
   "source": [
    "def cross_validation(dataset,cWord):\n",
    "    #print(dataset.shape[0])\n",
    "    foldSize = int(len(dataset)/5)\n",
    "    #print (foldSize)\n",
    "    #dataset_np = dataset.to_numpy()\n",
    "    d1 = dataset[0:foldSize]\n",
    "    #print(d1[\"Review\"])\n",
    "    d2 = dataset[foldSize:foldSize+1000]\n",
    "    \n",
    "    foldSize = foldSize + 1000\n",
    "    d3 = dataset[foldSize:foldSize+1000]\n",
    "    \n",
    "    foldSize = foldSize + 1000\n",
    "    d4 = dataset[foldSize:foldSize+1000]\n",
    "    \n",
    "    foldSize = foldSize + 1000\n",
    "    d5 = dataset[foldSize:foldSize+1000]\n",
    "    accuracies = []\n",
    "    words_in_review = []\n",
    "    folds = [d1,d2,d3,d4,d5]\n",
    "    for chunk in folds:\n",
    "        vocList = new_word_count(chunk)\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(\"No. of Words in dataset vocaublary list: \"+str(len(vocList)))\n",
    "        vocList = filterRare(vocList,5)\n",
    "        print(\"No. of Words, which have frequency > 5: \"+str(len(vocList)))\n",
    "        probability = classifier(vocList,'1',cWord,chunk)\n",
    "        target = '1' #positive sentiment\n",
    "        if probability < 0.50:\n",
    "            target = '0' #negative sentiment\n",
    "        correct = 0\n",
    "        #print(chunk)\n",
    "        for index, row in chunk.iterrows():\n",
    "            words_in_review = row[\"Review\"].split()\n",
    "            for word in words_in_review:\n",
    "                if(word == cWord and row[\"Class\"] == target):\n",
    "                    correct +=1\n",
    "                    break\n",
    "        #print(correct)\n",
    "        #print(chunk.shape[0])\n",
    "        accuracies.append(correct/chunk.shape[0])\n",
    "        \n",
    "    \n",
    "    #print(accuracies)\n",
    "    \n",
    "    return (sum(accuracies) / len(accuracies))\n",
    "\n",
    "print(\"K-fold cross validation with K as 5:\"+str(cross_validation(develop,'the')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effects of Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[the]: 19840/20000=0.992\n",
      "P[the]: 19841/46694, with smoothing= 0.4249154066903671\n",
      "-----------------------------------------------\n",
      "P[the|Positive ]: 9907/9995= 0.9911955977988994\n",
      "P[the|Positive ]: 9908/36689, with smoothing= 0.2700536945678541\n",
      "-----------------------------------------------\n",
      "P[the|Positive ]: 9907/9995= 0.9911955977988994\n",
      "P[the]: 19840/20000=0.992\n",
      "P[Positive|the]: 0.49934475806451617\n",
      "P[the|Positive ]: 9908/36689, with smoothing= 0.2700536945678541\n",
      "P[the]: 19841/46694, with smoothing= 0.4249154066903671\n",
      "P[Positive|the]: 0.31761460866326346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49975002499750026"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "\n",
    "calc_probability(\"the\",train,train_vocList)\n",
    "calc_probability_smoothing(\"the\",train,train_vocList,1)\n",
    "print(\"-----------------------------------------------\")\n",
    "conditional_probability('the',train,train_vocList,'1')\n",
    "conditional_probability_smoothing('the',train,train_vocList,'1',1)\n",
    "print(\"-----------------------------------------------\")\n",
    "classifier(train_vocList,'1','the',train)\n",
    "classifier_smoothing(train_vocList,'1','the',train,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 words that predict positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P[Positive| word] \n",
    "\n",
    "#using top 1000 words to find the top10 predictors of classes\n",
    "#this can be skipped, and the classifier can be run on entire vocabulary list, but that is time intensive task\n",
    "tempVocList = filterRare(train_vocList,400)\n",
    "posProbs = {}\n",
    "print(len(tempVocList))\n",
    "probability = 0\n",
    "\n",
    "for (key, value) in tempVocList.items():\n",
    "    #print(type(key))\n",
    "    #print(key)\n",
    "    probability = classifier(tempVocList,'1',key,train) #classifier(vocList,condition, cWord, dataset):\n",
    "    posProbs.update( {key : probability} )\n",
    "\n",
    "type(posProbs.items())\n",
    "#--------------------------------\n",
    "negProbs = {}\n",
    "\n",
    "probability = 0\n",
    "for (key, value) in tempVocList.items():\n",
    "    #print(type(key))\n",
    "    #print(key)\n",
    "    probability = classifier(tempVocList,'0',key,train) #classifier(vocList,condition, cWord, dataset):\n",
    "    negProbs.update( {key : probability} )\n",
    "\n",
    "##This was a time intensive task, even after filtering out words with less than 8000times occurence, i still had 40/50 words\n",
    "## which took considerable time to calculate.\n",
    "##However, given enough time it will calculate the top 1000 words\n",
    "\n",
    "\n",
    "#https://www.w3resource.com/python-exercises/dictionary/python-data-type-dictionary-exercise-1.php\n",
    "#Sorting posProbs & negProbs\n",
    "sorted_posProbs = dict(sorted(posProbs.items(), key=operator.itemgetter(1),reverse=True))\n",
    "#print('Dictionary in descending order by value : ',sorted_posProbs)\n",
    "\n",
    "sorted_negProbs = dict(sorted(negProbs.items(), key=operator.itemgetter(1),reverse=True))\n",
    "#print('Dictionary in descending order by value : ',sorted_negProbs)\n",
    "\n",
    "#https://stackoverflow.com/questions/29216889/slicing-a-dictionary\n",
    "import itertools\n",
    "\n",
    "posProbs_top10 = dict(itertools.islice(posProbs.items(), 10))\n",
    "negProbs_top10 = dict(itertools.islice(negProbs.items(), 10))\n",
    "\n",
    "print(\"Top 10 predictors for the positive class\")\n",
    "print(posProbs_top10)\n",
    "\n",
    "print(\"Top 10 predictors for the negative class\")\n",
    "print(negProbs_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
